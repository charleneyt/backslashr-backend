package jobs;

import flame.*;
import kvs.*;
import tools.*;
import tools.HTTP.*;
import java.net.*;
import java.util.regex.*;

import java.util.*;


public class Crawler {
  static boolean parsedBlackList = false;
  static List<String> blacklist = new ArrayList<>();

  public static void run (FlameContext ctx, String[] args) throws Exception {
    // Check whether the latter contains a single element (the seed URL), and output an error message (using the contextâ€™s output method) 
    // if it does not. If it does, output a success message, maybe "OK"
    System.out.println("executing crawler");
    if (args.length < 1 || args.length > 2){
        ctx.output("Invalid Argument! There must have one String for seed URL, and one optional argument for blacklist table name");
        return;
    }

FlameRDD urlQueue;
    
    if (args[0].startsWith("http")) {
        String[] parsedSeedUrl = parseURL(args[0]);
        if (parsedSeedUrl[0] == null && parsedSeedUrl[1] == null && parsedSeedUrl[2] == null){
          if (parsedSeedUrl[3].length() > 0 && !parsedSeedUrl[3].startsWith("/")){
            // default protocol to http if given link is not a relative one
            parsedSeedUrl[0] = "http";
            parsedSeedUrl[1] = parsedSeedUrl[3];
            parsedSeedUrl[3] = "/";
          } else {
            ctx.output("Invalid seed URL: missing protocol and host name");
            return;
          }
        }

        if (parsedSeedUrl[2] == null || parsedSeedUrl[2].length() == 0){
          parsedSeedUrl[2] = parsedSeedUrl[0].equals("http") ? "80" : "443";
        }
        if (parsedSeedUrl[3] == null){
          parsedSeedUrl[3] = "/";
        }

        String seedURL = parsedSeedUrl[0] + "://" + parsedSeedUrl[1] + ":" + parsedSeedUrl[2] + parsedSeedUrl[3];

        // for (String something : normalizeUrl(findUrl("<A  HREF=#abc></A> <a ref=lala hRef=\"blah.html#test\"></a><a HREF=\'../blubb/123.html\'>   \n<a href=/one/two.html></a><a></a><h1></h1><a href=http://elsewhere.com/some.html></a><a href=/SL7sCi.html>felt</a> <a href=https://foo.com/bar/xyz.html"), "https://foo.com:8000/bar/xyz.html")){
        //     System.out.println(something);
        // } 

        // if everything's OK, create an initial FlameRDD (perhaps called urlQueue) by parallelizing the seed URL
        urlQueue = ctx.parallelize(Arrays.asList(seedURL));
        // FlameRDD urlQueue = ctx.parallelize(Arrays.asList(args[0]));
    } else {
        urlQueue = ctx.fromTable(args[0], r -> r.get("value"));
    }

    // and then set up a while loop that runs until urlQueue.count() is zero
    while (urlQueue.count() > 0){
      // Within this loop, replace urlQueue with the result of a flatMap on itself. 
      // The flatMap will be running in parallel on all the workers, and each worker will be returning the (new) URLs it wants to put into the queue. In the lambda argument, take the URL, create a HttpURLConnection, set the request method to GET, use setRequestProperty to add the required header, and connect it; then check whether the response code is 200. If so, read the input stream, create a Row whose key is the hash of the URL, add the url and page fields, and use the putRow from KVSClient to upload it to the crawl table in the KVS. For now, return an empty list from the flatMap, so that the loop will terminate after a single iteration. 
      urlQueue = urlQueue.flatMap(url -> {
//        KVSClient kvs = FlameContext.getKVS();
        KVSClient kvs = new KVSClient("localhost:8000");
        System.out.println(url);
        
        // process blacklist if given argument is not null
        if (!parsedBlackList && args.length == 2){
          Iterator<Row> iter = kvs.scan(args[1], null, null);
          if (iter != null){
            while (iter.hasNext()){
              Row next = iter.next();
              if (next == null){
                break;
              }
              blacklist.add(next.get("pattern"));
            }
          }
          parsedBlackList = true;
        }

        String[] parsedUrl = parseURL(url);
        String hostHash = Hasher.hash(parsedUrl[1]);

        boolean robotReq = false;
        Row robotrow = null;
        // otherwise, check if robots.txt is downloaded
        if (kvs.existsRow("hosts", hostHash)){
          robotrow = kvs.getRow("hosts", hostHash);
          if (robotrow != null && robotrow.columns().contains("robotsReq")){
            // no need to make robots request again!
            robotReq = true;
          }
        }

        // if there's no robots column in hosts table with current host, make a GET request
        if (!robotReq){
          String robotsUrl = makeRobotsUrl(parsedUrl);
          Response r = HTTP.doRequest("GET", robotsUrl, null);
          kvs.put("hosts", hostHash, "robotsReq", String.valueOf(r.statusCode()).getBytes());
          if (r.statusCode() == 200 && r.body() != null){
            kvs.put("hosts", hostHash, "robots", r.body());
            robotReq = true;
          }
        }
        // if robotsReq exists, we already made requests before, no matter what status it returned
        // only when the req returns 200 do we put the body into robots column
        String robotTxt = "";
        if (robotReq && "200".equals(new String(kvs.get("hosts", hostHash, "robotsReq")))){
          robotTxt = new String(kvs.get("hosts", hostHash, "robots"));
        }

        // initialize rules and return list
        List<String> rules = parseRules(robotTxt);
        List<String> ret = new LinkedList<>();

        // only crawl data if passed the rule check
        if (checkRules(parsedUrl[3], rules)){
          String normalizedOriginal = normalizeImpl(url, null, new StringBuilder());
          if (normalizedOriginal == null || normalizedOriginal.length() == 0){
            return ret;
          }
          String urlHash = Hasher.hash(normalizedOriginal);
          // if already crawled (check url matches, and use responseCode as indicator)
          if (kvs.existsRow("crawl", urlHash)){
            Row r = kvs.getRow("crawl", urlHash);
            if (r.columns().contains("url") && r.get("url").equals(normalizedOriginal) && r.columns().contains("responseCode")){
              return ret;
            }
          }

          // sending HEAD request
          HttpURLConnection.setFollowRedirects(false);
          HttpURLConnection con = (HttpURLConnection)(new URL(url)).openConnection();
          con.setRequestMethod("HEAD");
          con.setRequestProperty("User-Agent", "cis5550-crawler");
          con.setInstanceFollowRedirects(false); // must set redirects to false!
          con.connect();
          int code = con.getResponseCode();
          String type = con.getContentType();
          int length = con.getContentLength();

          Row tempRow = new Row(urlHash);
          if (url != null){
            tempRow.put("url", url.getBytes());
          }
          
          if (code != 200){
            tempRow.put("responseCode", String.valueOf(code));

            switch (code){
              case 301:
              case 302:
              case 303:
              case 307:
              case 308:
                String redirect = con.getHeaderField("Location");
                if (redirect != null){
                  redirect = normalizeImpl(URLDecoder.decode(redirect, "UTF-8"), normalizedOriginal, new StringBuilder());
                  // if we have redirect from HEAD response, save the status code in crawl table and return the url from Location header
                  con.disconnect();

                  if (redirect != null && redirect.length() != 0){
                    tempRow.put("redirectURL", redirect);
                    if (!kvs.existsRow("crawl", urlHash)){
                      kvs.putRow("crawl", tempRow);
                    }
                    return Arrays.asList(redirect);
                  } else {
                    if (!kvs.existsRow("crawl", urlHash)){
                      kvs.putRow("crawl", tempRow);
                    }
                    return ret;
                  }
                }
                break;
            }
          }

          if (type != null){
            tempRow.put("contentType", type.getBytes());
          }
          if (length != -1){
            tempRow.put("length", String.valueOf(length).getBytes());
          }

          con.disconnect();


          // only issue GET if the HEAD response is 200 and type is text/html
          if (code == 200 && "text/html".equals(type)){
            // check if host has been called before, and if so, whether that was more than 1 second ago
            if (kvs.existsRow("hosts", hostHash)){
              Row hostRow = kvs.getRow("hosts", hostHash);
              if (hostRow != null && hostRow.columns().contains("time") && Long.valueOf(hostRow.get("time")) + 1000 >= System.currentTimeMillis()){
                return Arrays.asList(url);
              }
            }
            // update the hosts table's access time if it's last called more than 1 second ago
            kvs.put("hosts", hostHash, "time", String.valueOf(System.currentTimeMillis()).getBytes());

            // new GET connection only if responseCode is 200 and type is text/html
            con = (HttpURLConnection)(new URL(url)).openConnection();
            con.setRequestMethod("GET");
            con.setRequestProperty("User-Agent", "cis5550-crawler");
            con.setDoInput(true);
            con.connect();
            code = con.getResponseCode();
            tempRow.put("responseCode", String.valueOf(code).getBytes());

            byte[] response = con.getInputStream().readAllBytes();
            if (response != null){
              // EC 1 check if the content is duplicate with other crawled pages
              // use a table "ec1", where hash value of the String of content is row key, hash value of url is column key, and url is value
              String responseStr = new String(response);
              String responseHash = Hasher.hash(responseStr);
              boolean isDuplicate = false;
              if (kvs.existsRow("ec1", responseHash)){
                Row contentRow = kvs.getRow("ec1", responseHash);
                for (String contentUrl : contentRow.columns()){
                  String contentUrlHash = contentRow.get(contentUrl);
                  byte[] otherContent = kvs.get("crawl", contentUrlHash, "page");
                  if (contentUrl != null && otherContent != null && Arrays.equals(otherContent, response)){
                    tempRow.put("canonicalURL", contentUrl.getBytes());
                    isDuplicate = true;
                    break;
                  }
                }
              }
              
              if (!isDuplicate && response != null){
                tempRow.put("page", response); // only add "page" if the current page is not a duplicate!
                kvs.put("ec1", responseHash, url, urlHash.getBytes()); // if not a duplicate, save the current url's content hash to ec1 table
                for (String newUrl : findUrl(kvs, responseStr, normalizedOriginal, rules)){
                  ret.add(newUrl);
                }
              }
            }
            con.disconnect();
          }

          if (!kvs.existsRow("crawl", urlHash)){
            kvs.putRow("crawl", tempRow);
          }
        }
          // System.out.println(ret);
          return ret;
      });
      System.out.println("Finished a round");
      System.out.println("new table name: " + urlQueue.getTableName());
      System.out.println("new table count: " + urlQueue.count());
      Thread.sleep(1000);
    }
    Iterator<Row> iter = FlameContext.getKVS().scan("anchorEC", null, null);
    if (iter != null){
      while (iter.hasNext()){
        Row row = iter.next();
        if (row == null){
          break;
        }
        // if (FlameContext.getKVS().existsRow("crawl", row.key())){
          for (String colName : row.columns()) {
            FlameContext.getKVS().put("crawl", Hasher.hash(row.key()), colName, row.get(colName));
          }
        // }
      }
    }
    ctx.output("OK");
  }

  static List<String> findUrl(KVSClient kvs, String s, String originalUrl, List<String> rules) throws Exception {
    List<String> urls = new LinkedList<>();
    Matcher m = Pattern.compile("<[Aa][\\s\\S&&[^>]]*[Hh][Rr][Ee][Ff]=[\"\']?([\\S&&[^\"\'<>]]+)[\"\']?[\\s\\S&&[^>]]*>([\\s\\S&&[^<]]*)</[Aa]>")
                       .matcher(s);
    StringBuilder sb = new StringBuilder();
    Map<String, String> seen = new HashMap<>();
    while (m.find()){
      if (m.group(1) == null || m.group(1).length() == 0){
        continue;
      }
      String url = m.group(1);
      boolean unwantedType = false;
      int dot = url.lastIndexOf(".");
      if (dot != -1 && (dot == url.length()-4 || dot == url.length()-5)){
        switch (url.substring(dot).toLowerCase()){
          case ".jpg":
          case ".jpeg":
          case ".gif":
          case ".png":
          case ".txt":
            unwantedType = true;
            break;
        }
      }
      if (unwantedType){
        continue;
      }
      int sharpIdx = url.indexOf("#");
      if (sharpIdx != -1) {
        url = url.substring(0, sharpIdx);
        if (url.length() == 0) {
          continue;
        }
      }
      String copy = originalUrl;
      String normalizedUrl = normalizeImpl(url, copy, sb);
      if (normalizedUrl == null || normalizedUrl.length() == 0){
        continue;
      }

      // EC 2 if there's a blacklist, new url needs to be not matching with any given pattern
      if (checkRules(parseURL(normalizedUrl)[3], rules) && notBlacklisted(normalizedUrl, blacklist)){
        urls.add(normalizedUrl);

        // EC 3 anchor text extraction
        if (m.group(2) != null && m.group(2).length() > 0){
          String anchorTxt = m.group(2);
          
          // per Ed: don't need to eliminate duplicates - just concatenate the text from different anchors, with a space in between
          if (seen.containsKey(normalizedUrl)){
            seen.put(normalizedUrl, seen.get(normalizedUrl) + " " + anchorTxt);
          } else {
            seen.put(normalizedUrl, anchorTxt);
          }

          // if (kvs.existsRow("crawl", rowHash)){
          //   Row currRow = kvs.getRow("crawl", rowHash);
          //   if (currRow.columns().contains("anchor-" + normalizedUrl)){
          //     String existingAnchor = currRow.get(normalizedUrl);
          //     if (existingAnchor != null && existingAnchor.length() > 0){
          //       for (String a : existingAnchor.split(" ")){
          //         if (a.equals(anchorTxt)){
          //           anchorExists = true;
          //           break;
          //         }
          //       }
          //     }
          //     if (!anchorExists){
          //       anchorSb.append(existingAnchor);
          //     }
          //   }
          // }

          // if (!anchorExists){
          //   if (anchorSb.length() > 0){
          //     anchorSb.append(" " + anchorTxt);
          //   } else {
          //     anchorSb.append(anchorTxt);
          //   }
          //   // put the anchor text in crawl table under the found link, with column name anchor-url
          //   kvs.put("crawl", rowHash, "anchor-" + normalizedUrl, anchorSb.toString().getBytes());
          //   // if the crawl table has url now, update the anchor text
          //   // if (kvs.existsRow("crawl", rowHash)){
          //   //   System.out.println("putting anchor to crawl " + originalUrl);
          //   //   kvs.put("crawl", rowHash, "anchor", anchorSb.toString().getBytes());
          //   // }
          // }
        }
      }
    }
    for (String url : seen.keySet()){
      kvs.put("anchorEC", url, "anchor-" + originalUrl, seen.get(url).getBytes());
    }
    return urls;
  }

  static String normalizeImpl(String url, String originalUrl, StringBuilder sb){
    sb.setLength(0);
    if (url == null || url.length() == 0){
      return null;
    }
    
    if (url.startsWith("http://")) {
      sb.append(url.substring(0, 7));
      url = url.substring(7);
      int col = url.indexOf(":");
      if (col != -1 && col+1 < url.length() && Character.isDigit(url.charAt(col+1))) {
        sb.append(url);
      } else {
        if (url.contains("/")) {
          int slash = url.indexOf("/");
          sb.append(url.substring(0, slash));
          sb.append(":80");
          sb.append(url.substring(slash));
        } else {
          sb.append(url + ":80");
        }
      }
    } else if (url.startsWith("https://")) {
      sb.append(url.substring(0, 7));
      url = url.substring(8);
      int col = url.indexOf(":");
      if (col != -1 && col+1 < url.length() && Character.isDigit(url.charAt(col+1))) {
        sb.append(url);
      } else {
        if (url.contains("/")) {
          int slash = url.indexOf("/");
          sb.append(url.substring(0, slash));
          sb.append(":443");
          sb.append(url.substring(slash));
        } else {
          sb.append(url + ":443");
        }
      }
    } else if (!url.contains("://")) {
      if (originalUrl.startsWith("http://")) {
        sb.append(originalUrl.substring(0, 7));
        originalUrl = originalUrl.substring(7);
      } else {
        sb.append(originalUrl.substring(0, 8));
        originalUrl = originalUrl.substring(8);
      }
      if (url.startsWith("/")) {
        sb.append(originalUrl.substring(0, originalUrl.indexOf("/")));
        sb.append(url);
      } else {
        int lastSlash = originalUrl.lastIndexOf("/");
        if (lastSlash == -1){
          return null;
        }
        originalUrl = originalUrl.substring(0, lastSlash);

        while (url.startsWith("../")){
          lastSlash = originalUrl.lastIndexOf("/");
          if (lastSlash == -1){
            return null;
          }
          if (url.length() == 3){
            url = url.substring(2);
          } else {
            url = url.substring(3);
          }
          originalUrl = originalUrl.substring(0, lastSlash);
        }
        if ("..".equals(url)){
          return null;
        }

        sb.append(originalUrl);
        if (url.startsWith("/")){
          sb.append(url);
        } else {
          sb.append("/" + url);
        }
      }
    }
    return sb.toString();
  }

  static String[] parseURL(String url) {
    String result[] = new String[4];
    int slashslash = url.indexOf("//");
    if (slashslash>0) {
      result[0] = url.substring(0, slashslash-1);
      int nextslash = url.indexOf('/', slashslash+2);
      if (nextslash>=0) {
        result[1] = url.substring(slashslash+2, nextslash);
        result[3] = url.substring(nextslash);
      } else {
        result[1] = url.substring(slashslash+2);
        result[3] = "/";
      }
      int colonPos = result[1].indexOf(':');
      if (colonPos > 0) {
        result[2] = result[1].substring(colonPos+1);
        result[1] = result[1].substring(0, colonPos);
      }
    } else {
      if (url == null || url.length() == 0){
        result[3] = "/";
      } else {
        result[3] = url;
      }
    }

    return result;
  }

  static List<String> parseRules(String robot){
    if (robot.length() == 0){
      return new LinkedList<>();
    }
    String[] splitRobotTxt = robot.split("\n\n");

    List<String> self = new LinkedList<>();
    List<String> wildcard = new LinkedList<>();
    boolean selfExists = false;
    int idx = 0;
    while (idx < splitRobotTxt.length){
      if (splitRobotTxt[idx].startsWith("User-agent: cis5550-crawler\n")){
        Collections.addAll(self, splitRobotTxt[idx].split("\n"));
        self.remove(0);
        selfExists = true;
        break;
      } else if (splitRobotTxt[idx].startsWith("User-agent: *\n")){
        Collections.addAll(wildcard, splitRobotTxt[idx].split("\n"));
        wildcard.remove(0);
      }
      idx++;
    }
    if (selfExists){
      return self;
    } else {
      return wildcard;
    }
  }

  static boolean checkRules(String url, List<String> rules){
    if (url == null || url.length() == 0){
      return false;
    }
    for (String rule : rules){
      if (rule.startsWith("Allow: ")){
        if (url.startsWith(rule.substring(7))){
          break;
        }
      } else if (rule.startsWith("Disallow: ")){
        if (url.startsWith(rule.substring(10))){
          return false;
        }
      }
    }
    return true;
  }

  static String makeRobotsUrl(String[] parsed){
    StringBuilder sb = new StringBuilder();
    if (parsed[0] != null && parsed[0].length() > 0){
      sb.append(parsed[0] + "://");
    }
    sb.append(parsed[1]);
    if (parsed[2] != null && parsed[2].length() > 0){
      sb.append(":" + parsed[2]);
    }
    sb.append("/robots.txt");
    return sb.toString();
  }

  static boolean notBlacklisted(String url, List<String> blacklist){
    for (String bl : blacklist){
      bl = bl.replace("*", "\\S*");
      if (Pattern.matches(bl, url)){
        // we have a match! should ignore the given url
        return false;
      }
    }
    return true;
  }
}